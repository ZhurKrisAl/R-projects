---
title: "ПРОЕКТ_МСМ_2"
output: html_document
date: "2024-05-17"
editor_options: 
  markdown: 
    wrap: 72
---

```{r message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
library(magrittr)
library(openxlsx)
library(pander)
library(corrplot)
library(caret)
library(FactoMineR)
library(factoextra)
library(devtools)
library(rio)
library(psych)
library(ggpubr)
library(REdaS)
library(dplyr)
library(tidyr)
library(stringr)
library(cluster)
library(biotools)
library(pander)
library(tseries)
library(car)
library(lmtest)
```

Загрузим и посмотрим данные:

```{r}
df <- read.csv('bodyfat.csv')
str(df)
```

Удалим признак `Density`, так как зависимая переменная `BodyFat` входит
в формулу определения плотности тела и получается, что результаты будут
ненадёжными, если не уберём её из исследования.

```{r}
df <- df[-c(df$Density)]
```

> ## **1. Выделение главных компонент**

#### **1.0. Предварительный анализ**

Чтобы приступить к анализу, нам нужно убрать целевую переменную (в нашем
случае `BodyFat`) и отнормировать данные:

```{r}
df_new <- df[-c(1)] #убрали целевую переменную
```

```{r}
df_scaled <- scale(df_new) #нормируем данные
df_scaled <- as.data.frame(df_scaled)
```

Давайте, проведём предварительный визуальный анализ распределений
отнормированных данных:

```{r}
boxplot(df_scaled, xaxt = "n", cex.lab = 0.7)
text(x =  seq_along(names(df_new)), y = par("usr")[3] - 1, srt = 35, adj = 1,
     labels = names(df_new), xpd = TRUE, cex = 0.6)
```

Видим, что среднее значение данных приблизительно равно 0,
следовательно, можно предположить, что данные подчиняются нормальному
закону распределения.

Одним из самих важных условий построения МГК является
мультиколлинеарность, так как размерность можно снизить только в том
случае, когда признаки, описывающие совокупность, достаточно
коррелированы между собой:

```{r}
res <- cor.mtest(df_scaled, conf.level = 0.95)
corrplot(cor(df_scaled), p.mat = res$p, type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Как мы видим, большая часть признаков имеют сильную линейную связь. То
есть можно применить МГК. Но чтобы по-настоящему убедиться в этом, нам
необходимо провести более обоснованное тестирование - тест сферичности
Бартлетта:

```{r}
bart_spher(df_scaled)
```

Видим, что p-value значительно меньше 5% уровня значимости,
следовательно, нулевая гипотеза об ортогональности переменных
отвергается, то есть признаки достаточно коррелированны для того, чтобы
применять МГК.

Приступаем к МГК.

#### **1.1. Вывод о числе главных компонент, которые необходимо оставить для дальнейшего анализа с использованием: (a) критерия Кайзера; (b) доли суммарной вариации; (c) критерия каменистой осыпи**

Первым методом будет критерий Кайзера, согласно которому надо выбрать те
компоненты, где собственное значение больше 1:

```{r}
pc <- PCA(df_scaled, graph = FALSE)
pander(pc$eig)
```

Визуализируем:

```{r message=FALSE, warning=FALSE}
plot(eigen(cor(df_scaled))$values, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Собственное значение')
abline(h = 1, col = 'pink', lwd = 2)
text(x = 13, y = 1.2, 'eigen value = 1', col = 'pink')
```

Согласно данному методу, оптимальное число компонент - 3.

Второй метод - это определение главных компонентов с помощью доли
суммарной вариации, которую нужно сохранить на уровне 70-80%.

```{r message=FALSE, warning=FALSE}
princomp <- princomp(df_scaled, cor = TRUE)
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
abline(h = 70, col = 'black', lwd = 2)
abline(h = 80, col = 'pink', lwd = 2)
text(x = 13, y = 75, '70%', col = 'black')
text(x = 13, y = 85, '80%', col = 'pink')
```

Согласно данному методу оптимальное количество компонент - 2-3.

Последний метод - это "критерий каменистой осыпи", где нужно выбрать
такое число компонент, после которого происходит резкое падение доли
сохраненной дисперсии

```{r  message=FALSE, warning=FALSE}
fviz_eig(pc, addlabels = TRUE)
```

Согласно данному методу, оптимальное число компонент - 1.

На основании 3 тестов делаем вывод, что оптимальное число компонент - 3
(2 из 3 методов показали данное значение).

#### **1.2.-1.3. Описание суммарного вклада первых главных компонент. Построение графика накопленного вклада главных компонент в суммарную дисперсию исходного признакового пространства**

Собственные значения и доля суммарной вариации исходного набора
признаков, приходящаяся на главные компоненты:

```{r}
pander(pc$eig)
```

Согласно таблице первые 3 компоненты учитывают 79.98% дисперсии. Первая
компонента объясняет 61.85% дисперсии, вторая 10.42%, а третья
компонента - 7.71%. (? ПОДУМАТЬ НАД ВЫВОДОМ)

Также можно было бы воспользоваться библиотекой `stats`, функцией
`princomp` и получили бы те же самые значения:

```{r}
princomp <- princomp(df_new, cor = TRUE)
summary(princomp)
```

Теперь можем построить график накопленной дисперсии:

```{r  message=FALSE, warning=FALSE}
cumvarsum <- cumsum(princomp$sdev^2 / sum(princomp$sdev^2))*100
plot(cumvarsum, bstick = TRUE, type = 'b', main = '', xlab = 'Номер компоненты', 
     ylab = 'Кумулятивное значение вариации, %')
```

По графику тоже можно заметить, что после 3 компоненты график становится
гладким, что означает, что каждая последующая компонента будет выдавать
меньше вклад. (КАК БУДТО И ТАК ГЛАДКО?)

Теперь проверим все ли главные компоненты независимы:

```{r  message=FALSE, warning=FALSE}
main_comp <- c(1,2,3,4,5,6,7,8,9,10)
for(i in main_comp){
  for(j in i+1:3){
    pander(cor.test(princomp$scores[,i], princomp$scores[,j]))
  }
}
```

Как можно заметить, у всех пар главных компонент `p_value` равно 1,
следовательно, гипотеза о независимости не отвергается на 5% уровне
значимости, значит, главные компоненты попарно линейно независимы.
Также, это подтверждает невероятно маленькие значения корреляции.

#### **1.4. Интерпретация главных компонент на основе анализа матрицы факторных нагрузок. Дать названия выделенным главным компонентам**

Как уже ранее говорилось, оптимальное число компонент - 3. Давайте,
отобразим матрицу факторных нагрузок:

```{r message=FALSE, warning=FALSE}
pca <- principal(cor(df_scaled), nfactors = 3, rotate = "none", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 3, nrow = ncol(df_scaled))
rownames(pca_loadings) <- colnames(df_scaled)

kbl(round(pca_loadings, 3),
    caption = "Таблица 1. Матрица факторных нагрузок", 
    booktabs = T, col.names = c("PC1", "PC2", "PC3")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Применим варимакс-вращение и по данной матрице сделаем выводы:

```{r  message=FALSE, warning=FALSE}
pca <- principal(cor(df_scaled), nfactors = 3, rotate = "varimax", covar = FALSE)

pca_loadings <- matrix(as.numeric(pca$loadings), ncol = 3, nrow = ncol(df_scaled))
rownames(pca_loadings) <- colnames(df_scaled)

kbl(round(pca_loadings, 3),
    caption = "Таблица 2. Матрица факторных нагрузок с варимакс-вращением", 
    booktabs = T, col.names = c("PC1", "PC2", "PC3")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Факторные нагрузки показывают корреляцию между переменными и
компонентой. Чем больше связь между переменной и компонентой, к той
компоненте и относится переменная.

Первая компонента - Weight, Neck, Chest, Abdomen, Hip, Thigh, Knee,
Biceps, Wrist, Ankle, Forearm, Wrist. Вторая - Height. Третья - Age.
ЗДЕСЬ НАДО ЧЁ-ТО ПРИДУМАТЬ ПРО 1 ПЕРЕМЕННУЮ

Дадим названия нашим компонентам: 1. Измерения тела. 2. Рост. 3.
Возраст.

> ## **2. Построение уравнения регрессии с использованием выделенных ГК**

#### **2.1. Построение линейного уравнения регрессии на ГК**

```{r}
Q1 <- quantile(df$BodyFat, .25) 
IQR <- IQR(df$BodyFat)
df1 <- subset(df, df$BodyFat > (Q1 - 1.5*IQR)) #убираем выбросы
pc <- as.data.frame(pc$ind$coord[, 1:3])
pc1 <- pc$Dim.1 #выделяем 1-ую компоненту
pc2 <- pc$Dim.2 #выделяем 2-ую компоненту
pc3 <- pc$Dim.3 #выделяем 3-ю компоненту
lm_GK <- lm(df1$BodyFat ~ pc1 + pc2 + pc3) #построение регрессионной модели
pander(summary(lm_GK))
```

Данной моделью объясняется приблизительно 58% вариации зависимой
переменной исходя из значения Adjusted $R^2$. Также подмечаем, что
`p-value` всех компонент меньше 5% уровня значимости, что ещё раз
подтверждает, что они являются значимыми для модели.

#### **2.2. Сопоставление свойств ранее полученных уравнений регрессии (линейное и нелинейное уравнения регрессии) с уравнением регрессии на ГК**

Мы получаем 3 уравнения регрессии:

1)  Линейная регрессия (`lm1`):

$$ y_{BodyFat} = -21.54 + 0.07198 \cdot x_{Age} - 0.08279 \cdot x_{Weight} - 0.489 \cdot x_{Neck} + 0.9204 \cdot x_{Abdomen} - 0.2162 \cdot x_{Hip} + 0.3359 \cdot x_{Thigh} + 0.5175 \cdot x_{Forearm} - 1.511 \cdot x_{Wrist} +ε_i$$

Характеристика линейной регрессии:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
Q1 <- quantile(df$BodyFat, 0.25)
Q3 <- quantile(df$BodyFat, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

df2 <- df[df$BodyFat >= lower_bound & df$BodyFat <= upper_bound,]
lm1 <- lm(BodyFat ~ Age+Weight+Neck+Abdomen+Hip+Thigh+Forearm+Wrist, df2)
pander(summary(lm1))
```

2)  Нелинейная регрессия (`lm1_1`):

$$ y_{BodyFat} = -6.068 \cdot x_{Age}^{0.137}  \cdot x_{Weight}^{-0.4779} \cdot x_{Neck}^{-1.206}   \cdot x_{Abdomen}^{4.787}  \cdot x_{Hip}^{-2.116} \cdot x_{Thigh}^{1.307} \cdot x_{Forearm}^{0.696}\cdot x_{Wrist}^{-1.482}\cdotε_i$$

```{r, echo=FALSE, message=FALSE, warning=FALSE}
df3 = apply(df, 1, function(row) all(row != 0 ))
df3_1 <- df[df3,]
df3_22 <- log(df3_1)
Q1 <- quantile(df3_22$BodyFat, 0.25)
Q3 <- quantile(df3_22$BodyFat, 0.75)
IQR <- Q3 - Q1

lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

df3_2 <- df3_22[df3_22$BodyFat >= lower_bound & df3_22$BodyFat <= upper_bound,]

lm1_1 <- lm(BodyFat ~ Age + Weight + Neck + Abdomen + Hip + Thigh + Forearm + Wrist, df3_2)
pander(summary(lm1_1))
```

3)  Уравнение регрессии на ГК (`lm_GK`):

$$ y_{BodyFat} = 19.15 + 1.829 \cdot Component_{1} + 2.831 \cdot Component_{2}  - 1.718  \cdot Component_{3}  +ε_i$$

1)  Проверим подчиняются ли остатки моделей нормальному распределению c
    помощью теста Харке-Бера:

**`lm1`:**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
pander(jarque.bera.test(lm1$residuals))
```

**`lm1_1`:**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
pander(jarque.bera.test(lm1_1$residuals))
```

**`lm_GK`:**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
pander(jarque.bera.test(lm_GK$residuals))
```

По результатам всех тестов `p-value` больше 5%-го уровня значимости,
следовательно, гипотеза о подчинении остатков к нормальному закону
распределения **не отвергается**.

2)  Проверка остатков на независимость с помощью теста Дарбина-Ватсона:

**`lm1`:**

```{r}
durbinWatsonTest(lm1) 
```

**`lm1_1`:**

```{r}
durbinWatsonTest(lm1_1)
```

**`lm_GK`:**

```{r}
durbinWatsonTest(lm_GK)
```

Во всех моделях выполнена гипотеза о некоррелируемости остатков, так как
`p-value` больше 5%-го уровня значимости.

3)  Проверка на гомоскедастичность с помощью графиков:

**`lm1`:**

```{r}
res <- resid(lm1)
plot(fitted(lm1),res)
abline(0,0)
```

**`lm1_1`:**

```{r}
res <- resid(lm1_1)
plot(fitted(lm1_1),res)
abline(0,0)
```

**`lm_GK`:**

```{r}
res <- resid(lm_GK)
plot(fitted(lm_GK),res)
abline(0,0)
```

| Регрессионная модель | Линейная зависимость | Нормальность остатков | Независимость остатков | Гомоскедастичность остатков | Adjusted $R^2$ |
|----------------------|----------------------|-----------------------|------------------------|-----------------------------|----------------|
| `lm1` (линейная)     | ✔️                   | ✔️                    | ✔️                     | ✔️                          | 72.9%          |
| `lm1_1` (нелинейная) | ✖️                   | ✖️                    | ✔️                     | ✖️                          | 65.5%          |
| `lm_GK` (на ГК)      | ✔️                   | ✔️                    | ✔️                     | ✔️                          | 57.9%          |

Гетероскедастичность определяет равномерно ли разбросаны остатки. Из-за
присутствия гетероскедастичности данных результатам регрессионного
анализа трудно доверять. На графике для степенной модели мы видим
закономерность - остатки становятся менее разбросанными с ростом
поборанного значения, такая форма разброса - признак
гетероскедастичсноти, поэтому лучше не использовать степенную модель.
Перейдем к сравнению линейной регрессионной модели и регрессионной
модели на главных компонентам с помощью информационного критерия.

САШААА ПОСМОТРИ ИСПРАВЬ И ДОБАВЬ ПРО R2 У ГК И ЛИНЕЙНОЙ И СДЕЛАЙ
ПРЕДВАРИТЕЛЬНЫЙ ВЫВОД ЧТО ЛИНЕЙНАЯ ЛУЧШЕ

#### **2.3. Выбор и обоснование лучшего уравнения**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
IC_table <- data.frame(n = c('lm1', 'lm_GK'), 
                  a = rbind(AIC(lm1), AIC(lm_GK)), 
                  b = rbind(BIC(lm1), BIC(lm_GK)))
kbl(IC_table,
    caption = "Таблица 3. Информационные критерии Акаике и Шварца (Баесовский инф. критерий)", 
    booktabs = T, col.names = c("Модель", "Значение AIC", "Значение BIC")) %>% 
    kable_classic(html_font = "Cambria", font_size = 12, full_width = F)
```

Получаем, что для модели линейной регрессии `lm1` информационные
критерии Акаике и Шварца немного меньше, чем для модели `lm_GK`. Значит
, лучше использовать модель линейной регрессии из компьютерной работы 1.

> ## **3. Кластерный анализ**

Перед проведением кластеризации выделим 4 наблюдения из датасета, чтобы
потом использовать их в дискриминантном анализе:

```{r}
set.seed(1)
rand_df <- df[sample(nrow(df), size = 4), ]
rand_df
```

Удалим эти наблюдения из основного датафрейма и масштабируем их:

```{r}
df_del <- df[-c(249, 68, 167, 129),]
df_del_scaled <- as.data.frame(scale(df_del[-c(1)]))
```

#### **3.1. Построение и анализ дендрограмм**

Отбор классифицирующих признаков:

```{r}
corrplot(cor(scale(df_new)), type = "full", method = "circle", tl.col = "black", tl.srt = 45, tl.cex = 0.5)
```

Для опредления оптимального числа кластеров воспользуемся разными
способами.

1)  Метод локтя (elbow method):

```{r}
fviz_nbclust(df_del_scaled , kmeans, method = 'wss') +
  labs(x = 'Число кластеров', y = 'Сумма внутрикластерных дисперсий',
       title = 'Зависимость WSS от числа кластеров')
```

В данном случае выбираем 2 кластера, так как у второго кластера прирост
суммы внутрикластерной дисперсии резко сокращается.

2)  Метод силуэтов (silhouettes):

```{r}
fviz_nbclust(df_del_scaled, kmeans, method = 'silhouette') +
  labs(x = 'Число кластеров', y = 'Средняя ширина силуэта по всем точкам',
       title = 'Зависимость средней ширины силуэта от числа кластеров')
```

Чем ближе средняя ширина силуэта по всем точкам к 1, тем лучше. В данном
случае наибольшее значение будет достагиться в точке с двумя (2)
кластерами.

3)  Статистикой разрыва (Gap-статистика):

```{r}
fviz_nbclust(df_del_scaled, kmeans, method = 'gap_stat') +
  labs(x = 'Число кластеров', y = 'Статистика разрыва',
       title = 'Зависимость статистики разрыва от числа кластеров')
```

При двух кластерах статистика разрыва меньше, чем в случае с одним
кластером, поэтому согласно этому методу лучше отказаться от деления
данных на кластеры.

Получаем, что два теста указывают на разбиение на два кластера, а по
итогам последнего от деления на кластеры лучше отказаться.

```{r}
mahalnobis <- sqrt(D2.dist(df_del_scaled, cov(df_del_scaled)))

hclust <- hclust(mahalnobis, method = 'ward.D2')
hclust$cluster <- cutree(hclust, k = 2)

fviz_dend(hclust, cex = 0.5,  k = 2, color_labels_by_k = TRUE, 
          main = 'Дендрограмма, расстояние Махаланобиса (принцип Варда)', ylab = 'Расстояние')
```

Разделение на кластеры по полученному графику некорректно, так как в
результате мы получаем два кластера, один из которых включает в себя
только одно значения, в то время как остальные данные находятся во
втором кластере. Причиной такого разделения может оказаться однородность
данных и некорректность проведения иерархической кластеризации при
большом объеме выборки. Таким образом, от кластеризации лучше
отказаться. +ПОДПИШИ ПЖ ЧТО У ТЕБЯ ПО ДРУГИМ ВООБЩЕ НИЧЕ НЕ СТРОИЛОСЬ

> ## **4. Использование метода к-средних для классификации объектов**

#### **4.1. Построение и анализ графика средних значений показателей в кластерах**

Определение количества кластеров.

В кластерном анализе гиперпараметром является число кластеров, то есть
для анализа нам необходимо заранее знать их число. Для определения
количества кластеров воспользуемся тем опитимальным количеством, которое
получили в прошлом пункте, а именно, 2 кластера.

Приступим к использованию метода k-средних. Для начала важно
зафиксировать значения для их неизменности при различных запусках,
поскольку это может привести к некоторым ошибкам. Именно поэтому
дополнительно к методу пропишем строку 'set.seed', чтобы программа
выдавала одинаковые значения при различных запусках. Также, поскольку мы
определили оптимальным числом 2 кластера, пропишем это и в нашей
функции, то есть установим centers = 2 (два центра кластеров).

```{r}
set.seed(1)
kmeans2 <- kmeans(df_del_scaled, centers = 2)
kmeans2
```

В начале таблицы фиксируются расчитанные центры кластеров, далее векторы
принадлежностей к кластерам, а затем разница между разделенными
кластерами. В результате разбиения заметим, что 1-ый кластер имеет
размер в 144 наблюдения, а 2-ой кластер - 104 наблюдения.

Теперь мы визуализируем результаты кластеризации через график средних и
с его помощью дадим интерпретацию полученным кластерам:

```{r}
plot(1:ncol(df_del_scaled), kmeans2$centers[1,], xaxt = 'n', type = 'l', col = 'blueviolet', 
     cex.sub = 0.3, cex.lab = 0.8, lwd = 2, ylim = c(-3, 3), 
     ylab = 'Среднее значение признака', xlab = 'Классифицирующий признак')

lines(1:ncol(df_del_scaled), kmeans2$centers[2,], type = 'l', col = 'darkslategray4', lwd = 2)

title('График средних (признаки стандартизованы)')
legend(0.8, 5, c('Кластер 1', 'Кластер 2'),
       lwd = c(2, 2), col = c('blueviolet','darkslategray4'))
text(x =  seq_along(names(df_del_scaled)), y = par("usr")[3] - 1, srt = 15, adj = 0.5,
     labels = names(df_del_scaled), xpd = TRUE, cex = 0.5)
```

По графику видны некоторые интересные особенности двух кластеров:

-   Средние значения кластеров для признака `Age` практически равны, что
    позволяет сделать вывод о том, что, вероятно, признак возраста очень
    слабо влияет на всю классификацию, что подтверждается и
    корреляционной матрицей, в которой значения парных корреляций
    возраста с остальными признаками были в основном незначительными.
-   Значения средних в кластерах по подавляющему большинству признаков
    находятся на примерно равном расстоянии, что говорит нам о схожих
    значениях различий по этим признакам в данных кластерах.
-   Значения средних в кластерах по признаку `Height` также очень близки
    к друг другу (намеренно взят малый масштаб, чтобы четче были видны
    различия в средних, поэтому значения на графике действительно очень
    близки друг к другу), что говорит нам о том, что признак роста слабо
    влияет на всю классификацию, что подтверждается и корреляционной
    матрицей, в которой значения парных корреляций роста с остальными
    признаками были в основном незначительными.

#### **4.2. Проверка гипотезы о равенстве средних значений в кластерах**

Тест ANOVA.

Для того, чтобы убедиться в наших выводах, следующим шагом мы проведем
тест ANOVA. Поскольку нулевой гипотезой данного теста выступает
равенство всех средних значений, то мы будем обращать внимание на
значение p-value \< 0.05 (меньшее условного уровня значимости в 5%),
чтобы зафиксировать, в каких случаях средние всех уровней фактора
различны.

```{r}
pander(summary(aov(df_del_scaled$Age ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Weight ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Height ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Neck ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Chest ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Abdomen ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Hip ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Thigh ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Knee ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Ankle ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Biceps ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Forearm ~ as.factor(kmeans2$cluster))))
```

```{r}
pander(summary(aov(df_del_scaled$Wrist ~ as.factor(kmeans2$cluster))))
```

В результате теста ANOVA делаем вывод, что практически во всех тестах
p-value \< 0.05, кроме переменной `Age`, из чего следует, что с
вероятностью ошибки 5% нулевая гипотеза о равенстве средних всех уровней
фактора отвергается в пользу альтернативной гипотезы о том, что между
некоторыми уровнями (хотя бы одним из уровней) есть различие в средних
значениях. Поскольку у переменной `Age` `p-value=0.7405` больше уровня
значимости в 5% (как и любого другого адекватного уровня), то можем
сказать, что между всеми уровнями фактора возраста отсутствуют различия
в средних значениях. Стоит также отметить, что тест ANOVA показал
различия в уровнях фактора `Height` (нулевая гипотеза о равенстве была
отвергнула в силу `p-value=0.002073` \< 0.05), это значит, что данный
признак все же влияет на классификацию, хотя и незначительно.

#### **4.3. Интерпретация полученных результатов (необходимо дать названия кластерам и обосновать их выбор)**

Мы предполагаем, что именно целевая переменная - `BodyFat` является
ключевым обоснованием в делении на кластеры, то есть именно она делит
все данные в массиве на две категории показателей: "высокий процент
жира", "низкий процент жира". Это связано с высокой степенью
коррелированности этой переменной с остальными признаками:

```{r}
dff <- cbind(df_del, cluster = kmeans2$cluster) #необходимо загрузить значения кластерных векторов в исходный массив данных
cor(dff$BodyFat, dff$cluster)                #проверяем корреляцию между кластерами и переменной BodyFat
```

Корреляция равная \~0.5 по модулю является умеренной, что подкрепляет
наши предположения о возможности разделения наблюдений на кластеры по
признаку веса. Положительное значение говорит нам о прямой взаимосвязи
между весом и остальными показателями: 1-"низкий процент жира",
2-"высокий процент жира", а не наоборот.

#### **4.4. Описание кластеров с помощью графических средств, помогающих обосновать название кластеров**

Дополнительно проиллюстрируем кластеры для более наглядного отображения
и дальнейших выводов с помощью главных компонент.

```{r}
fviz_cluster(kmeans2, data = df_del_scaled)
```

Вывод: на графике хорошо заметно, что два кластера плотно пересекаются,
что может говорить о НЕ самом корректном разбиении на кластеры.
Поскольку большинство точек сосредоточено в диапазоне значений (-5;5) по
оси абсцисс, корректное разбиение на кластеры представляется достаточно
проблематичным. Точки образуют практически единое плотное облако, именно
поэтому тест статистики разрыва показал эффективность разделения на 1
кластер: разрывы между точками в облаке крайне малы. Однако также
заметим, что разбиение на большее количество кластеров (больше 2) слабо
представляется возможным, с левой стороны графика наблюдается более
выраженный разброс рассредоточенных точек, именно поэтому наблюдения
разделены на 2 кластера. Такой вид облака точек обоснован тем, что
большинство значений сосредоточено относительно природной нормы веса, в
центре распределения на диаграмме, то есть, как уже было прописанно во 2
пункте, данные подчиняются нормальному закону распределения и тяжело
поддаются делению на обособленные четкие кластеры.

> ## **5. Построение регрессионных моделей в кластерах (типологическая регрессия)**

#### **5.1. Построение уравнений регрессии в кластерах**

Итак, мы разделили выборку на два кластера и нам необходимо для каждого
кластера построить своё уравнение регрессии с теми же переменными, что и
входили в уравнение регрессии для всей выборки.

Для начала необходимо разделить выборку на 2 кластера:

```{r}
class_1 <- subset(dff[dff$cluster == 1, ], select = -c(cluster)) #выделили 1-ый класс и удалили столбец со значениями кластеров
class_2 <- subset(dff[dff$cluster == 2, ], select = -c(cluster)) #выделили 2-ой класс и удалили столбец со значениями кластеров
```

Теперь переходим к построению уравнений регрессии для каждого класса:

```{r}
lm_class_1 <- lm(BodyFat ~ Age+Weight+Neck+Abdomen+Hip+Thigh+Forearm+Wrist, data = class_1) #регрессионная модель для 1 класса
pander(summary(lm_class_1))
```

В модели для 1-го класса значимой переменной на уровне 5% является
`Abdomen` и `Wrist`, то есть мы можем сказать, что существует
статистически значимая связь между `Bodyfat` и `Wrist` (процентом жира в
организме и окружностью запястья; `Bodyfat` и `Abdomen` (процентом жира
и окружностью живота), что также логично. Скорректированное значение
коэффицента детирминации уменьшилось и стало равно 61%.

Построение регрессии для 2-го класса:

```{r}
lm_class_2 <- lm(BodyFat ~ Age+Weight+Neck+Abdomen+Hip+Thigh+Forearm+Wrist, data = class_2) 
pander(summary(lm_class_2))
```

В модели для 2-го класса переменные `Weight` и `Abdomen` являются
значимыми на уровне 5%, то есть мы можем сказать, что существует
статистически значимая связь между `Bodyfat` и `Weight` (процентом жира
в организме и весом тела); `Bodyfat` и `Abdomen` (процентом жира и
окружностью живота), что логично. Так же, посмотрев на скорректированное
значение коэффициента детерминации, мы делаем вывод, что 70% вариации
значений процента жира объясняются данной моделью.

#### **5.2. Сопоставление и интерпретация коэффициентов регрессии в кластерах с использованием коэффициентов эластичности**

Уравнение регрессии для 1 класса:
$$ y_{BodyFat} = - 26.9 + 0.07794 \cdot x_{Age} - 0.0315  \cdot x_{Weight} - 0.5515 \cdot x_{Neck} + 0.9105 \cdot x_{Abdomen} - 0.07732 \cdot x_{Hip} + 0.3111 \cdot x_{Thigh} + 0.507 \cdot x_{Forearm} - -2.159 \cdot x_{Wrist} +ε_i$$

Уравнение регрессии для 2 класса:
$$ y_{BodyFat} = - 24.63 + 0.06082 \cdot x_{Age} - 0.1154 \cdot x_{Weight} -0.2704 \cdot x_{Neck} + 0.9564 \cdot x_{Abdomen} - -0.2222 \cdot x_{Hip} + 0.2803 \cdot x_{Thigh} + 0.4856 \cdot x_{Forearm} - -1.362  \cdot x_{Wrist} +ε_i$$

Для расчёта коэффициентов эластичности необходимо знать предсказанные
значения моделей, поэтому сначала сделаем предсказания:

```{r}
class_1$y_pred = predict(lm_class_1)
class_2$y_pred = predict(lm_class_2)
```

Как мы знаем, коэффициент эластичности показывает на сколько процентов в
среднем изменится значени результативного признака `BodyFat` при
изменении факторного показателя на 1%. Поэтому будем делать выводы
исходя из этого знания, а именно, чем меньше значение коэффициента, тем
меньшее влияние оказывает признак на результативный показатель.

```{r}
paste('Коэффициент эластичности для `Age` класс 1 =', round(0.07794 * mean(class_1$Age)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Age` класс 2 =', round(0.06082 * mean(class_2$Age)/mean(class_2$y_pred), 2), '%')
```

Наблюдаем, что признак `Age` оказывает незначительное положительное
влияние на изменение результативного показателя, но всё-таки более
сильное воздействие оказано на 1-ый класс.

```{r}
paste('Коэффициент эластичности для `Weight` класс 1 =', round(-0.0315 * mean(class_1$Weight)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Weight` класс 2 =', round(-0.1154 * mean(class_2$Weight)/mean(class_2$y_pred), 2), '%')
```

Признак `Weight` оказывает незначительное отрицательное влияние на
изменение результативного показателя, но всё-таки более сильное
воздействие оказано на 2-ой класс. При увеличении веса на 1%, процент
жира в среднем снижается на 0.32% в 1 классе и на 0.99% во 2 классе.
Возможно, это связано с ростом мышечной массы.

```{r}
paste('Коэффициент эластичности для `Neck` класс 1 =', round(-0.5515 * mean(class_1$Neck)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Neck` класс 2 =', round(-0.2704 * mean(class_2$Neck)/mean(class_2$y_pred), 2), '%')
```

Признак `Neck` оказывает малое отрицательное влияние на изменение
результативного показателя, но всё-таки более сильное воздействие
оказано на 1-ый класс. При увеличении обхвата шеи на 1%, процент жира в
среднем снижается на 1.3% в 1 классе и на 0.45% во 2 классе. Это также
может быть связано с ростом мышечной массы.

```{r}
paste('Коэффициент эластичности для `Abdomen` класс 1 =', round(0.9105 * mean(class_1$Abdomen)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Abdomen` класс 2 =', round(0.9564 * mean(class_2$Abdomen)/mean(class_2$y_pred), 2), '%')
```

Признак `Abdomen` оказывает положительное влияние на изменение
результативного показателя и тут изменения процента жира более значимы,
но всё-таки более сильное воздействие оказано на 1-ый класс. При
увеличении обхвата живота на 1%, процент жира в среднем увеличивается на
5.04% в 1 классе и на 4.05% во 2 классе, что логично, ведь прибавление
объёмов в данной части тела действительно связано с ростом жира.

```{r}
paste('Коэффициент эластичности для `Hip` класс 1 =', round(-0.07732 * mean(class_1$Hip)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Hip` класс 2 =', round(-0.2222 * mean(class_2$Hip)/mean(class_2$y_pred), 2), '%')
```

Признак `Hip` оказывает незначительное отрицательное влияние на
изменение результативного показателя. Более сильное влияние оказывается
на класс 2. При увеличении объёмов бёдер на 1%, процент жира в среднем
снижается на 0.48% в 1 классе и на 0.98% во 2 классе, возможно, это тоже
следствие роста именно мышечной массы.

```{r}
paste('Коэффициент эластичности для `Thigh` класс 1 =', round(0.3111 * mean(class_1$Thigh)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Thigh` класс 2 =', round(0.2803 * mean(class_2$Thigh)/mean(class_2$y_pred), 2), '%')
```

Признак `Thigh` оказывает положительное влияние на результативный
показатель. Более сильное влияние оказано на 1-ый класс. При увеличении
объёма ляжек (часть ноги выше голени) на 1%, процент жира в среднем
увеличивается на 1.13% в 1 классе и на 0.74% во 2 классе.

```{r}
paste('Коэффициент эластичности для `Forearm` класс 1 =', round(0.507 * mean(class_1$Forearm)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Forearm` класс 2 =', round(0.4856 * mean(class_2$Forearm)/mean(class_2$y_pred), 2), '%')
```

Признак `Forearm` оказывает положительное влияние на результативный
показатель. Более сильное воздействие на 1 класс. При увеличении объёмов
в предплечье на 1%, процент жира в среднем увеличивается на 0.9% в 1
классе и на 0.61% во 2 классе.

```{r}
paste('Коэффициент эластичности для `Wrist` класс 1 =', round(-2.159 * mean(class_1$Wrist)/mean(class_1$y_pred), 2), '%')
paste('Коэффициент эластичности для `Wrist` класс 2 =', round(-1.362  * mean(class_2$Wrist)/mean(class_2$y_pred), 2), '%')
```

Признак `Wrist` оказывает отрицательное влияние на изменение
результативного показателя. Замечаем, что более сильное воздействие на
класс 1. При увеличении обхвата запястья на 1%, процент жира в среднем
снижается на 2.46% в 1 классе и на 1.07% во 2 классе.

Итак, давайте, упорядочим признаки по воздействию на изменение
результативного показателя в каждом классе. Начнеём с более активного
влияния (1) и закончим менее влиятельным признаком (8).

| Класс | *1*       | *2*     | *3*      | *4*     | *5*       | *6*       | *7*      | *8*   |
|-------|-----------|---------|----------|---------|-----------|-----------|----------|-------|
| 1     | `Abdomen` | `Wrist` | `Neck`   | `Thigh` | `Forearm` | `Hip`     | `Weight` | `Age` |
| 2     | `Abdomen` | `Wrist` | `Weight` | `Hip`   | `Thigh`   | `Forearm` | `Neck`   | `Age` |

Видим, что и в 1 и во 2 классе самый сильно воздействующий признак -
`Abdomen` и самый слабо воздействующий на результат - `Age`. Если мы
спроецируем наши результаты на реальную жизнь, то это так, высокие
значения обхвата живота действительно связаны с высоким процентом жира и
в свою очередь возраст почти никак не влияет на значения процента жира.
Что интересно, `Weight` с 3 места во 2 классе перешёл на предпоследнее в
1 классе, что ещё раз подкрепляет наши суждения о том, что в 1 класс
входят люди с низким процентом жира.

#### **5.3. Сопоставление качества построенных моделей в кластерах и для всей совокупности объектов в целом**

Сначала вспомним характеристику линейной регрессии по всей совокупности:

```{r}
pander(summary(lm1))
```

Теперь выведем характеристики моделей для каждого класса.

Характеристика линейной регрессии для 1-го класса:

```{r}
pander(summary(lm_class_1))
```

Характеристика линейной регрессии для 2-го класса:

```{r}
pander(summary(lm_class_2))
```

1)  Проверим остатки на нормальность распределения с помощью теста
    Шапиро-Уилка:

```{r}
pander(shapiro.test(lm1$residuals)) #остатки линейной регрессионной модели
```

```{r}
pander(shapiro.test(lm_class_1$residuals)) #остатки линейной регрессионной модели 1-го класса
```

```{r}
pander(shapiro.test(lm_class_2$residuals)) #остатки линейной регрессионной модели 2-го класса
```

Итак, мы видим, что p-value везде больше 5% уровня значимости, значит,
гипотеза о нормальном распределении остатков не отвергается.

2)  Проверка остатков на независимость с помощью теста Дарбина-Ватсона:

```{r}
durbinWatsonTest(lm1) #регрессионная модель для всей совокупности
```

```{r}
durbinWatsonTest(lm_class_1) #регрессионная модель для 1-го класса
```

```{r}
durbinWatsonTest(lm_class_2) #регрессионная модель для 2-го класса
```

Здесь мы наблюдаем, что у моделей для всей совокупности и для 2-го
класса p-value больше 5% уровня значимости, что говорит о том, что
остатки являются независимыми. Для остатков регрессионной модели 1-го
класса гипотеза о независимости остатков отвергается, значит, они
зависимы.

3)  Проверка остатков на гомоскедастичность c помощью теста
    Бреуша-Пагана:

```{r}
bptest(lm1)
```

```{r}
bptest(lm_class_1)
```

```{r}
bptest(lm_class_2)
```

Итак, везде p-value больше 5% уровня значимости, значит, гипотеза о
гомоскедастичности остатков не отвергается.

Давайте сделаем сводную таблицы и подведём итоги о качестве моделей.

| Регрессионная модель           | Линейная зависимость | Нормальность остатков | Независимость остатков | Гомоскедастичность остатков | Adjusted $R^2$ |
|--------------------------------|----------------------|-----------------------|------------------------|-----------------------------|----------------|
| `lm1` (для всей совокупности)  | ✔️                   | ✔️                    | ✔️                     | ✔️                          | 72.9%          |
| `lm_class_1` (для 1-го класса) | ✔️                   | ✔️                    | ✖️                     | ✔️                          | 61.3%          |
| `lm_class_2` (для 2-го класса) | ✔️                   | ✔️                    | ✔️                     | ✔️                          | 70.2%          |

Итак, смотря на таблицу понимаем, что лучшая по качеству модель -
линейная регрессионная модель для всей выборки. Во-первых, у неё
выполнены все предпосылки регрессионной модели, что говорит о надёжности
результатов, но данные предпосылки выполняются и у модели для 2-го
класса. Во-вторых, смотрим на скорректированный коэффициент
детерминации, который больше всех представленных, что говорит, что
моделью `lm1` объясняется почти 73% разброса значений процентов жира.

Также, мы вспомнили про критерий Акаике и Шварца, но в данном разделе он
не применим, так как AIC и BIC зависят от размера выборок, а они здесь
разные, что приведёт к ошибочным выводам.

> ## **6. Дискриминантный анализ**

#### **6.1. Построение дискриминантных функций. Выводы о качестве модели**

Для проведения дискриминантного анализа используем результаты k-means из
пункта 4. Там было получено 2 кластера.

```{r}
set.seed(1)
kmeans2 <- kmeans(df_del_scaled, centers = 2)
kmeans2
```

Присоединим вектор значений принадлежности к кластеру к основным уже
отмасштабированным данным:

```{r}
clusters <- kmeans2$cluster
df_6 <- as.data.frame(cbind(df_del_scaled, clusters))
```

Делим выборку на обучающую (2/3) и тестовую (1/3):

```{r}
smpl_size <- floor(2/3 * nrow(df_6))

set.seed(1)
train_ind <- sample(seq_len(nrow(df_6)), size = smpl_size)

data.train <- as.data.frame(df_6[train_ind,])
data.test <- as.data.frame(df_6[-train_ind,])
data.test
```

Строим дискриминантную функцию:

```{r, warning = FALSE, message = FALSE}
lda.fit <- lda(data.train[,-c(14)], data.train$clusters) #-c(14) - убираем 14 колонку с номерами кластеров
lda.fit
```

"Prior probabilities of groups" - "Априорные вероятности группы": -
59,4% всех наблюдений в обучающей выборке относятся к 1 классу; - 40.6%
всех наблюдений в обучающей выборке относятся ко 2 классу. В итоге была
получена 1 дискриминантная функция, которая обеспечивает 100% разделение
данных на кластеры. 2 кластера, поэтому 1 LD.

Графически:

```{r}
plot(lda.fit)
```

По графику видим, что группы имеют разный разброс значений, но при этом
есть пересечение в 0, следовательно, можно предварительно сказать, что
эти группы отличимы друг от друга и нормальное разбиение на кластеры
возможно, но не утверждать.

Прогнозы на тестовой выборке:

```{r}
lda.pred <- predict(lda.fit, data.test[,-c(14)])
```

Гистограмма значений дискриминантной функции:

```{r}
ldahist(data = lda.pred$x[,1], g = clusters)
```

Наблюдаем, что значения для разных групп пересекаются, что может
сформировать ненадёжные выводы.

Перейдем к лямбде Уилкса, которая показывает, значимо ли различаются
между собой средние значения дискриминантной функции в исследуемых
группах:

```{r}
ldam <- manova(as.matrix(data.test[,-c(14)]) ~ lda.pred$class)

pander(summary(ldam, test = "Wilks"))
```

Видим, что `p-value` приблизительно 0, значит, при сравнении с любым
разумным уровнем значимости, гипотеза о сходстве средних отвергается, то
есть средние исследуемых групп значимо различаются, из чего следует
доказательство о наличии дискриминирующих особенностей переменных и о
качественном разделении данных по классам. Также мы убеждаемся в этом,
когда смотрим на `лямбду Уилкса = 0.2736`, значение которой близко к 0.

#### **6.2. Отнесение новых объектов (3-4 наблюдения) к выделенным и описанным кластерам различными способами с использованием ДФ**

Отобранные раннее (в 3 пункте) наблюдения (новые объекты):

```{r}
set.seed(1)
rand_df <- df[sample(nrow(df), size = 4), ]
rand_df
```

Давайте, удалим целевую переменную и отмасштабируем:

```{r}
rand_df_new <- scale(rand_df)
rand_df_new <- data.frame(rand_df_new)
rand_df_new <- rand_df_new[-c(1)]
rand_df_new
```

Теперь сделаем прогноз для этих наблюдений при помощи дискриминатной
функции, построенной на основе метода k-means:

```{r}
lda4.pred <- predict(lda.fit, rand_df_new)
lda4.pred <- data.frame(lda4.pred)
head(lda4.pred)
```

Исходя из апостериорных вероятностей каждое наблюдение было определено в
какой-то класс. Интересно, что наблюдение 129 имеет вероятность 50/50 по
распределению в 1 и 2 класс, но всё-таки немного перевесил 2 класс.

Давайте посмотрим на значения процента жира для каждого наблюдения:

```{r}
rand_df[c('BodyFat')]
```

Видим, что наблюдение 249 имеет 33.6% жира и его предсказанный класс - 2
("высокий процент жира"); 68 имеет 13.8% жира и его класс - 1 ("низкий
процент жира"); 167 имеет 21.8% жира и его класс - 1 ("низкий процент
жира") и 129 имеет 20.8% жира и его класс - 2 ("высокий процент жира").
Заметим, что 167 наблюдение имеет процент жира выше, чем 129 наблюдение
и при этом 167 наблюдение попало в класс с низким процентом жира, а 129
наоборот с высоким, и вспомним, про пограничное состояние наблюдения
129, значит, всё-таки данное наблюдение было неправильно
классифицировано. В остальном, по смыслу всё определено верно.

#### **6.3. Уточнение результатов классификации, выполненной с помощью метода к-средних, с помощью аппарата дискриминантного анализа (выявление некорректно классифицированных наблюдений)**

Давайте посмотрим как тестовые данные распределены по классам:

```{r}
table(data.test$clusters)
```

Видим, что в классе 1 - 46 наблюдений, в классе 2 - 37.

Далее посмотрим на распределение прогнозов, сделанных на тестовой
выборке, по классам:

```{r}
summary(lda.pred$class)
```

Видим, что в классе 1 - 47 наблюдений, в классе 2 - 36. Понимаем, что
есть предсказанный класс не соответствует истинному.

Матрица ошибок:

```{r}
table(lda.pred$class, data.test[,c("clusters")])
```

Итак, 4 наблюдения были отнесены ко 2 классу, хотя их истинный класс -
1; 3 наблюдения были отнесены к 1 классу, хотя их истинный - 2; то есть
всего 7 некорректно классифицированных наблюдений.

Давайте посмотрим на выявленные некорректно классифицированные
наблюдения:

```{r}
which(data.test$clusters != lda.pred$class)
```

Видим, что наблюдения 3, 5, 6, 36, 47, 69, 76 были неправильно
классифицированны.

#### **6.4. Анализ классификационной матрицы (classification matrix). Вывод о качестве разбиения объектов на кластеры**

Таблица соответствия предсказанных классов исходным:

```{r}
misclass <- function(pred, obs) { tbl <- table(pred, obs)
sum <- colSums(tbl)
dia <- diag(tbl)
msc <- ((sum - dia)/sum) * 100
m.m <- mean(msc)
cat("Classification table:", "\n")
print(tbl)
cat("Misclassification errors:", "\n")
print(round(msc, 2))

print(round(m.m, 2))}

misclass(lda.pred$class, data.test[,c("clusters")])
```

Снова видим матрицу ошибок (Classification table), которая была описана
в предыдущем пункте. Также Misclassification errors, по которым делаем
вывод, что 6.52% наблюдений 1-го класса были ошибочно отнесены ко 2 и
10.81% наблюдений 2-го класса были ошибочно отнесены к 1. В целом, 8.67%
наблюдений от всей выборки были отнесены к ошибочному классу. Ошибка не
очень высокая, что указывает на хорошую предсказывающую способность
модели, но всё зависит от задачи, возможно, для какого-то случая данная
ошибка модели будет критичной.

#### **6.5. Построение графика принадлежности тестовой и тренировочной выборок к кластерам по результатам проведенного анализа**

```{r}
plot(lda.fit)
```

```{r}
ldahist(data = lda.pred$x[,1], g = clusters)
```
